{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librairies Importing & Dataset Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('merged_dataset_VF03.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_yes_no_columns(df):\n",
    "    yes_no_columns = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        unique_values = set(df[col].astype(str).str.lower())\n",
    "        if 'yes' in unique_values and 'no' in unique_values:\n",
    "            yes_no_columns.append(col)\n",
    "    \n",
    "    return yes_no_columns\n",
    "\n",
    "yes_no_cols = find_yes_no_columns(df)\n",
    "print(f\"Columns with 'yes'/'no' values: {yes_no_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    if df[column].isin(['Yes', 'No']).all(): \n",
    "        df[column] = df[column].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns\n",
    "for column in categorical_columns:\n",
    "    print(f\"Column: {column}\")\n",
    "    value_counts = df[column].value_counts()\n",
    "    print(\"Unique values and frequencies:\")\n",
    "    print(value_counts)\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['Gender'] = label_encoder.fit_transform(df['Gender'])\n",
    "df['Contract'] = label_encoder.fit_transform(df['Contract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_mean_target_encoding(df, feature, target):\n",
    "    \n",
    "    mean_target = df.groupby(feature)[target].mean()\n",
    "    \n",
    "    category_counts = df[feature].value_counts()\n",
    "    \n",
    "    weighted_mean = mean_target * category_counts / category_counts.sum()\n",
    "    \n",
    "    df[feature] = df[feature].map(weighted_mean)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = weighted_mean_target_encoding(df, 'Offer', 'Churn Value')\n",
    "df = weighted_mean_target_encoding(df, 'Internet Type', 'Churn Value')\n",
    "df = weighted_mean_target_encoding(df, 'Payment Method', 'Churn Value')\n",
    "\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.select_dtypes(include=[np.number])\n",
    "X = X.drop(columns=['Churn Value'])\n",
    "\n",
    "z_scores = np.abs((X - X.mean()) / X.std(ddof=0))\n",
    "threshold = 3\n",
    "outliers = z_scores > threshold\n",
    "print(f\"Outliers detected per column:\\n{outliers.sum(axis=0)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_visualize = [\n",
    " 'Total Refunds',\n",
    " 'Total Extra Data Charges',\n",
    " 'Total Revenue',\n",
    " 'Total Long Distance Charges',\n",
    " 'Avg Monthly GB Download'\n",
    "]\n",
    "\n",
    "df_selected = df[columns_to_visualize]\n",
    "\n",
    "skewness = df_selected.skew()\n",
    "print(\"Skewness of selected columns:\")\n",
    "print(skewness)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_log_transform = [\n",
    "    'Total Revenue', \n",
    "    'Total Long Distance Charges', \n",
    "    'Avg Monthly GB Download'\n",
    "]\n",
    "\n",
    "for column in columns_to_log_transform:\n",
    "    df[column] = np.log(df[column] + 1)  \n",
    "\n",
    "print(df[['Total Revenue', \n",
    "          'Total Long Distance Charges', \n",
    "          'Avg Monthly GB Download']].skew())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_boxcox = [\n",
    "    'Total Refunds',\n",
    "    'Total Extra Data Charges'\n",
    "]\n",
    "\n",
    "for column in columns_to_boxcox:\n",
    "    if (df[column] <= 0).any():\n",
    "        df[column] = df[column] + 1\n",
    "    df[column], _ = stats.boxcox(df[column])\n",
    "\n",
    "print(df[['Total Refunds',\n",
    "    'Total Extra Data Charges']].skew())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tenure-Based Features\n",
    "\n",
    "Tenure Group: Categorizing customers based on tenure:\n",
    "\n",
    "New (0-6 months)\n",
    "\n",
    "Short-term (7-12 months)\n",
    "\n",
    "Medium-term (13-24 months)\n",
    "\n",
    "Long-term (>24 months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tenure Group'] = pd.cut(df['Tenure in Months'], \n",
    "                            bins=[0, 6, 12, 24, df['Tenure in Months'].max()], \n",
    "                            labels=['New', 'Short-term', 'Medium-term', 'Long-term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "df['Tenure Group'] = encoder.fit_transform(df['Tenure Group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total Extra Charges Ratio'] = df['Total Extra Data Charges'] / (df['Total Charges'] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Security Bundle'] = (df['Online Security'] + df['Online Backup'] + \n",
    "                         df['Device Protection Plan'] + df['Premium Tech Support'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Multiple Services Score'] = (df['Phone Service'] + df['Multiple Lines'] + \n",
    "                                 df['Streaming TV'] + df['Streaming Movies'] + \n",
    "                                 df['Streaming Music'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "correlated_features = set()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.85:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            rowname = correlation_matrix.columns[j]\n",
    "            correlated_features.add((colname, rowname))\n",
    "\n",
    "print(\"Correlated Features (correlation > 0.85):\")\n",
    "for pair in correlated_features:\n",
    "    print(pair)\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Dependents'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Scaling/Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "binary_cols = [col for col in numeric_cols if df[col].nunique() == 2]\n",
    "\n",
    "continuous_numeric_cols = [col for col in numeric_cols if col not in binary_cols]\n",
    "\n",
    "print(\"Continuous Numerical Features:\", continuous_numeric_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_standardize = [\n",
    "    'Tenure in Months', 'Offer', 'Avg Monthly Long Distance Charges', \n",
    "    'Internet Type', 'Avg Monthly GB Download', 'Payment Method', \n",
    "    'Total Extra Data Charges', 'Total Long Distance Charges', \n",
    "    'Satisfaction Score', 'CLTV', 'Age', 'Number of Dependents'\n",
    "]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "df[columns_to_standardize] = scaler.fit_transform(df[columns_to_standardize])\n",
    "\n",
    "print(\"Standardized Features:\")\n",
    "print(df[columns_to_standardize].describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "correlated_features = set()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.85:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            rowname = correlation_matrix.columns[j]\n",
    "            correlated_features.add((colname, rowname))\n",
    "\n",
    "print(\"Correlated Features (correlation > 0.85):\")\n",
    "for pair in correlated_features:\n",
    "    print(pair)\n",
    "\n",
    "plt.figure(figsize=(30, 30))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(‘Total Revenue’, ‘Tenure in Months’)\n",
    "Reason for Correlation: Customers who have stayed longer (higher Tenure in Months) tend to generate higher Total Revenue.\n",
    "\n",
    "Which to Keep?\n",
    "Drop Total Revenue, Keep Tenure in Months\n",
    "\n",
    "Tenure in Months is a more fundamental feature for churn prediction.\n",
    "\n",
    "Total Revenue is derived from tenure and charges, so it's redundant.\n",
    "_______________________________\n",
    "\n",
    "(‘Number of Dependents’, ‘Dependents’)\n",
    "Reason for Correlation: Number of Dependents is a numerical count, while Dependents is usually a binary indicator (Yes/No).\n",
    "\n",
    "Which to Keep?\n",
    "Drop Dependents, Keep Number of Dependents\n",
    "\n",
    "Number of Dependents has more information (it provides exact numbers rather than just Yes/No).\n",
    "\n",
    "If Dependents is already binary (0 for No, 1 for Yes), it's less useful than an exact count.\n",
    "_____________________________\n",
    "\n",
    "(‘Total Revenue’, ‘Total Charges’)\n",
    "Reason for Correlation:\n",
    "\n",
    "Total Revenue is often the sum of Total Charges and other components.\n",
    "\n",
    "If Total Revenue = Total Charges + Extra Fees, one is redundant.\n",
    "\n",
    "Which to Keep?\n",
    "Drop Total Revenue, Keep Total Charges\n",
    "\n",
    "Total Charges reflects how much the customer has been billed, which can directly influence churn.\n",
    "\n",
    "Total Revenue might include additional elements that don’t add much predictive value.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Dependents', 'Total Revenue'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Churn Value\"])\n",
    "y = df[\"Churn Value\"]\n",
    "\n",
    "model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "n_features_to_select = 10\n",
    "rfe = RFE(estimator=model, n_features_to_select=n_features_to_select)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "ranking_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Ranking\": rfe.ranking_,\n",
    "    \"Selected\": rfe.support_\n",
    "}).sort_values(by=\"Ranking\")\n",
    "\n",
    "print(\"\\nRFE Feature Ranking:\")\n",
    "print(ranking_df.to_string(index=False))\n",
    "\n",
    "selected = ranking_df[ranking_df[\"Selected\"]][\"Feature\"].tolist()\n",
    "print(f\"\\nTop {len(selected)} Selected Features:\")\n",
    "for i, feature in enumerate(selected, 1):\n",
    "    print(f\"{i}. {feature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Gender','Total Charges', 'Monthly Charge', 'Total Refunds' ], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'o-', markersize=4)\n",
    "plt.title('Scree Plot: Variance Explained by Each Principal Component')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', markersize=4)\n",
    "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For determining the optimal number of components, I'd recommend:\n",
    "\n",
    "From the scree plot: The elbow appears to be around 5-6 components. After this point, each additional component adds much less new information.\n",
    "From the cumulative variance plot: With about 10 components, you're capturing approximately 80% of the variance, and with 15 components you're at roughly 90-92%.\n",
    "\n",
    "The optimal number depends on your specific goals:\n",
    "\n",
    "If you need to be very strict about dimensionality reduction, 5-6 components would be reasonable as that's where the elbow appears most pronounced.\n",
    "If you want to ensure you capture most of the variance, 10-15 components would be appropriate.\n",
    "\n",
    "A common compromise is to select the number of components that explains 80-90% of the variance, which in your case would be around 10-12 components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "eigenvalues = pca.explained_variance_\n",
    "\n",
    "n_components_kaiser = sum(eigenvalues > 1)\n",
    "print(f\"Number of components according to Kaiser's Rule: {n_components_kaiser}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(eigenvalues) + 1), eigenvalues)\n",
    "plt.axhline(y=1, color='r', linestyle='--', label='Eigenvalue = 1')\n",
    "plt.xlabel('Component Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.legend()\n",
    "plt.title('Scree Plot with Kaiser\\'s Rule')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "pca_final = PCA(n_components=n_components_kaiser)\n",
    "X_pca = pca_final.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# X_pca now contains your transformed data with 10 components\n",
    "print(f\"Original data shape: {X.shape}\")\n",
    "print(f\"Reduced data shape: {X_pca.shape}\")\n",
    "\n",
    "# To see how much variance is explained by these 10 components\n",
    "print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {np.sum(pca.explained_variance_ratio_)}\")\n",
    "\n",
    "# If you want to know which original features contribute most to each component\n",
    "# Look at the components (loadings)\n",
    "feature_names = X.columns if hasattr(X, 'columns') else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "components_df = pd.DataFrame(pca.components_.T, index=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, component in enumerate(pca.components_):\n",
    "    sorted_indices = np.argsort(np.abs(component))[::-1]\n",
    "    print(f\"\\nTop 5 features in PC{i+1}:\")\n",
    "    for idx in sorted_indices[:5]:\n",
    "        print(f\"{feature_names[idx]}: {component[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline as imPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('V2_ZscoreScaled.csv')\n",
    "X = df.drop(['Churn Value'], axis=1)\n",
    "y = df['Churn Value']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_original, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_original) \n",
    "X_train_pca = pca.transform(X_train_original)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "def train_evaluate_svc(X_train, X_test, y_train, y_test, model_name):\n",
    " \n",
    "    baseline_pipeline = imPipeline(steps=[\n",
    "        ('classifier', SVC(\n",
    "            kernel='poly',\n",
    "            degree=2,\n",
    "            C=0.5, \n",
    "            gamma='scale',  \n",
    "            probability=True  \n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    baseline_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_baseline_pred = baseline_pipeline.predict(X_test)\n",
    "    y_baseline_proba = baseline_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_baseline_pred), 2),\n",
    "        \"ROC AUC\": round(roc_auc_score(y_test, y_baseline_proba), 2),\n",
    "        \"Log Loss\": round(log_loss(y_test, y_baseline_proba), 2),\n",
    "        \"F1 Score\": round(f1_score(y_test, y_baseline_pred), 2),\n",
    "        \"Precision\": round(precision_score(y_test, y_baseline_pred), 2),\n",
    "        \"Recall\": round(recall_score(y_test, y_baseline_pred), 2)\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Evaluation:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_baseline_pred))\n",
    "    print(f\"{model_name} modeling complete.\\n\")\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_baseline_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"No Churn\", \"Churn\"], yticklabels=[\"No Churn\", \"Churn\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics, baseline_pipeline\n",
    "\n",
    "original_metrics, original_model = train_evaluate_svc(X_train_original, X_test, y_train_balanced, y_test, \"Original SVC Model\")\n",
    "\n",
    "pca_metrics, pca_model = train_evaluate_svc(X_train_pca, X_test_pca, y_train_balanced, y_test, \"PCA-Based SVC Model\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Original Model\": original_metrics,\n",
    "    \"PCA Model\": pca_metrics\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('Performance Comparison: Original vs PCA-based SVC Model')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline as imPipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv('V2_ZscoreScaled.csv')\n",
    "X = df.drop(['Churn Value'], axis=1)\n",
    "y = df['Churn Value']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.333, random_state=42)\n",
    "\n",
    "print(\"Before SMOTE:\", Counter(y_train))\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_original, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE:\", Counter(y_train_balanced))\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "pca.fit(X_train_original) \n",
    "X_train_pca = pca.transform(X_train_original)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "def train_evaluate_rf(X_train, X_test, y_train, y_test, model_name):\n",
    " \n",
    "    baseline_pipeline = imPipeline(steps=[\n",
    "        ('classifier', RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    baseline_pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    y_baseline_pred = baseline_pipeline.predict(X_test)\n",
    "    y_baseline_proba = baseline_pipeline.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    metrics = {\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_baseline_pred), 2),\n",
    "        \"ROC AUC\": round(roc_auc_score(y_test, y_baseline_proba), 2),\n",
    "        \"Log Loss\": round(log_loss(y_test, y_baseline_proba), 2),\n",
    "        \"F1 Score\": round(f1_score(y_test, y_baseline_pred), 2),\n",
    "        \"Precision\": round(precision_score(y_test, y_baseline_pred), 2),\n",
    "        \"Recall\": round(recall_score(y_test, y_baseline_pred), 2)\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name} Evaluation:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_baseline_pred))\n",
    "    print(f\"{model_name} modeling complete.\\n\")\n",
    "    \n",
    "    conf_matrix = confusion_matrix(y_test, y_baseline_pred)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[\"No Churn\", \"Churn\"], yticklabels=[\"No Churn\", \"Churn\"])\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.title(f\"Confusion Matrix - {model_name}\")\n",
    "    plt.show()\n",
    "    \n",
    "    return metrics, baseline_pipeline\n",
    "\n",
    "original_metrics, original_model = train_evaluate_rf(X_train_original, X_test, y_train_balanced, y_test, \"Original RF Model\")\n",
    "\n",
    "pca_metrics, pca_model = train_evaluate_rf(X_train_pca, X_test_pca, y_train_balanced, y_test, \"PCA-Based RF Model\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Original Model\": original_metrics,\n",
    "    \"PCA Model\": pca_metrics\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "comparison_df.plot(kind='bar')\n",
    "plt.title('Performance Comparison: Original vs PCA-based Random Forest Model')\n",
    "plt.ylabel('Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
